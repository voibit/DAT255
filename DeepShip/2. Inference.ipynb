{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5468ee0e-a1df-4cab-b21c-e88892704283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path, PurePath\n",
    "import math\n",
    "from fastai.vision.all import *\n",
    "import torchaudio\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91091004-cb94-4001-943f-3c4f4500534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out fft sizes\n",
    "# STFT LF\n",
    "sr = 32000\n",
    "imgsize = 460\n",
    "Fmax = sr / 2\n",
    "Nfft_lf = 32768\n",
    "\n",
    "Fbin_lf = Fmax / Nfft_lf\n",
    "Nskip_lf = Nfft_lf //5  # 20% overlap  #int(round((sr * 60 - Nfft_lf) / (imgsize))) # makes a minute \n",
    "rng_lf = Nskip_lf * (imgsize-1)\n",
    "       \n",
    "stft_lf = torchaudio.transforms.Spectrogram(Nfft_lf,hop_length=Nskip_lf, power=2,return_complex=False ).cuda()\n",
    "\n",
    "# STFT HF\n",
    "Nfft_hf = 1024\n",
    "Fbin_hf = Fmax / Nfft_hf\n",
    "Nskip_hf = 1392 #int(round((sr * 60 - Nfft_hf*2) / imgsize)/2 ) # half a minute\n",
    "rng_hf = Nskip_hf * imgsize + Nfft_hf\n",
    "stft_hf = torchaudio.transforms.Spectrogram(Nfft_hf,hop_length=Nskip_hf, power=2,return_complex=False ).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2fe7849-4ec5-482e-8bf3-cdd5b0c3cf67",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2551211368.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [21]\u001b[0;36m\u001b[0m\n\u001b[0;31m    with tempfile.TemporaryFile as tmp\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryFile as tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec66771f-e630-4ed3-88b7-49d1d8bde4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch learner for batch predictions\n",
    "def predict_batch(self, item, rm_type_tfms=None, with_input=False):\n",
    "    dl = self.dls.test_dl(item, rm_type_tfms=rm_type_tfms, num_workers=0)\n",
    "    ret, _,mask = self.get_preds(dl=dl, with_decoded=True)\n",
    "    return ret, mask\n",
    "Learner.predict_batch = predict_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f041553e-566e-48ce-8948-73a81786c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readWav(p: Path, rand=True):\n",
    "    frames = torchaudio.info(p).num_frames\n",
    "    last = frames-rng_lf\n",
    "    wav = torch.Tensor()  \n",
    "    # Repeat wav if not long enough\n",
    "    while last < 0:\n",
    "        wav = torch.cat((wav,torchaudio.load(p)[0]),1)\n",
    "        last += frames\n",
    "    # Random start point\n",
    "    start = random.randint(0,last) if rand else int(last/2)\n",
    "    \n",
    "    # If enough frames\n",
    "    if frames-rng_lf > 0: \n",
    "        return torchaudio.load(p, num_frames=rng_lf, frame_offset=start)[0]\n",
    "    else: \n",
    "        wav = torch.cat((wav,torchaudio.load(p)[0]),1)\n",
    "        return wav[:,start:start+rng_lf]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25c7af9f-649e-4848-b058-ef0e5f29feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normSamp(audio):\n",
    "    ret = audio - torch.mean(audio)\n",
    "    return ret / torch.max(ret)\n",
    "\n",
    "def normSpec(spec):\n",
    "    # take the logarithm of the values\n",
    "    ret = torch.log10(spec+1e-20)\n",
    "    mean = torch.mean(ret)\n",
    "    std = torch.std(ret)\n",
    "    # Normalize each frame so its max 1, we dont need the extra dimension\n",
    "    #return (ret / torch.transpose(torch.max(ret,2)[0],0,1))[0]\n",
    "    #return (ret / torch.max(ret))[0]\n",
    "    \n",
    "    ret =  (ret - mean) / (std*4) + 0.5\n",
    "    return torch.clamp(ret, min=0, max=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a746f16-a9e9-45d9-9305-db84724f19b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavToSpecs(wavs : torch.Tensor, hf_idx=0):\n",
    "    lf = stft_lf(wavs)[0]\n",
    "    \n",
    "    lf0 = normSpec(lf[:imgsize,:imgsize])\n",
    "    lf1 = normSpec(lf[imgsize:imgsize*2,:imgsize])\n",
    "    \n",
    "    hf = stft_hf(wavs[:,hf_idx:hf_idx+rng_hf])[0]\n",
    "    \n",
    "    hf = normSpec(hf[12:imgsize+12,:imgsize])\n",
    "    #return torch.stack((normSpec(lf[0, ]),mf, hf),0)\n",
    "    return Spectrogram.create(torch.stack((lf0,lf1,hf),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7cedfa2-a32a-4498-9a6a-f9c62551fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spectrogram(TensorImageBase):\n",
    "    \"\"\"Type to represent a spectogram which knows show itself\"\"\"\n",
    "    @classmethod\n",
    "    def create(cls, o: Tensor):\n",
    "        return cls(o)\n",
    "    \n",
    "    def show(self, figsize=None, ctx=None, **kwargs): \n",
    "        channels = self.shape[0]\n",
    "        t = self\n",
    "        if not isinstance(t, Tensor): return ctx\n",
    "        if figsize is None: figsize=(10,10)   \n",
    "        #f, axarr = plt.subplots(1,3,figsize=(15,15))\n",
    "        #axarr[0].imshow(specs[0,:,:].cpu(),extent=[0,imgsize,(imgsize+1)*Fbin_lf,Fbin_lf] ,aspect=1/Fbin_lf)\n",
    "        #axarr[1].imshow(specs[1,:,:].cpu(),extent=[0,imgsize,(imgsize+50)*Fbin_hf,Fbin_hf*50] ,aspect=1/Fbin_hf)\n",
    "        #axarr[2].imshow(specs[2,:,:].cpu())\n",
    "        return show_images(t, nrows=1, ncols=channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e37e879-a77b-42f3-bcad-d5f5212f95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a fastai Transform\n",
    "class SpectrogramTransform(RandTransform):\n",
    "    \"A transform handler for multiple `spect` transforms\"\n",
    "    split_idx,order=None,0  # 0 = HIGH prio\n",
    "    #def __init__(self, train_aug, valid_aug): store_attr()\n",
    "    def __init__(self): \n",
    "        store_attr()\n",
    "    \n",
    "    def before_call(self, b, split_idx):\n",
    "        self.idx = split_idx\n",
    "    \n",
    "    def encodes(self, p : Path):\n",
    "        \n",
    "        if self.idx == 0: #Train transform\n",
    "            hf_idx = random.randint(0,rng_lf-rng_hf)\n",
    "            wav = readWav(p, True)\n",
    "        else: #Valid transform\n",
    "            hf_idx = (rng_lf-rng_hf) //2\n",
    "            wav = readWav(p, False)\n",
    "        return wavToSpecs(wav.cuda(), hf_idx)\n",
    "    \n",
    "    #def decodes(self, x): return TitledImage(x,'test')\n",
    "def get_wavs(p : Path) :\n",
    "    return get_files(p,'.wav')\n",
    "#class ImgTransform(ItemTransform):\n",
    "#    def __init__(self, vocab): self.vocab = vocab\n",
    "#    def encodes(self, o): return o\n",
    "    #def decodes(self, x): return TitledImage(x[0],self.vocab[x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f52862c7-3f36-4115-835e-a8bfeb96922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_func(p : Path):\n",
    "    if PurePath(p).parent.name == \"AmbientSE\": return []\n",
    "    return [PurePath(p).parent.parent.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a314a51-e7a8-4242-b234-1ac5bbfad569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predTensor(p, overlap=0.5):\n",
    "    frames = torchaudio.info(p).num_frames\n",
    "    wav,sr = torchaudio.load(p)\n",
    "    \n",
    "    # Resample audio if different samplerate\n",
    "    if(sr != 32000): \n",
    "        print('resampling to 32000..')\n",
    "        wav = torchaudio.functional.resample(wav, sr, 32000)\n",
    "        sr = 32000\n",
    "        frames = wav.shape[1]\n",
    "        print('resampling done..')\n",
    "    # Repeat wav if not long enough\n",
    "    \n",
    "    while wav.shape[1]-rng_lf < 0 :\n",
    "        wav = torch.cat((wav,wav),1)\n",
    "        \n",
    "    spec = wavToSpecs(wav[:,0:rng_lf].cuda())[None,:,:,:]\n",
    "    # cat rest of the frames\n",
    "    \n",
    "    hf_idx = (rng_lf-rng_hf) //2\n",
    "    \n",
    "    start = int(Nskip_lf *(1-overlap) * (imgsize))\n",
    "    \n",
    "    runs = int((frames-Nfft_lf)/(Nskip_lf*(1-overlap))/imgsize) +1 \n",
    "    for i in range(1,runs):\n",
    "        idx = start*i\n",
    "        if idx+rng_lf > frames: # add last frame \n",
    "            spec = torch.cat((spec,wavToSpecs(wav[:,-rng_lf:].cuda(),hf_idx)[None,:,:,:]),0)\n",
    "            break\n",
    "        # Add frame according to index\n",
    "        spec = torch.cat((spec,wavToSpecs(wav[:,idx:idx+rng_lf].cuda(),hf_idx)[None,:,:,:]),0)\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5b3b60a-5a05-4655-9ab3-6d2e52f339a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = '../DeepShip/roro.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebc5cef8-72f3-4fd0-bcdc-580c41f84c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "formats: can't open input file `../DeepShip/roro.wav': No such file or directory\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error loading audio file: failed to open file ../DeepShip/roro.wav",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m specBatch \u001b[38;5;241m=\u001b[39m \u001b[43mpredTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mpredTensor\u001b[0;34m(p, overlap)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredTensor\u001b[39m(p, overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     frames \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnum_frames\n\u001b[1;32m      3\u001b[0m     wav,sr \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mload(p)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Resample audio if different samplerate\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/torchaudio/backend/sox_io_backend.py:53\u001b[0m, in \u001b[0;36minfo\u001b[0;34m(filepath, format)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m AudioMetaData(\u001b[38;5;241m*\u001b[39msinfo)\n\u001b[1;32m     52\u001b[0m     filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(filepath)\n\u001b[0;32m---> 53\u001b[0m sinfo \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msox_io_get_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m AudioMetaData(\u001b[38;5;241m*\u001b[39msinfo)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error loading audio file: failed to open file ../DeepShip/roro.wav"
     ]
    }
   ],
   "source": [
    "specBatch = predTensor(testfile, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33a00887-3b84-4bd3-befd-bf425608b938",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'specBatch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mspecBatch\u001b[49m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'specBatch' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(specBatch[1,0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8299a04-bfed-4b28-84b4-074404fdf372",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner('../models/resnet50-93')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c54f6e2-f2d5-4b77-8c3c-6b1208058132",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'specBatch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m learn\u001b[38;5;241m.\u001b[39mpredict_batch(\u001b[43mspecBatch\u001b[49m); preds\n",
      "\u001b[0;31mNameError\u001b[0m: name 'specBatch' is not defined"
     ]
    }
   ],
   "source": [
    "preds = learn.predict_batch(specBatch); preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d732f20-4fc3-4714-a0b7-cb6bf9f25e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(pbatch):\n",
    "    p = torch.mean(pbatch[0]* (pbatch[1]+ 0.0* ~pbatch[1]) ,0).tolist()\n",
    "    return {k: v for k,v in zip(learn.dls.vocab, p)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d74cd7-1932-4a48-bfb4-8020210bec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcafbb2-714c-44aa-aef1-55f0ab2a88d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289b92e-e791-4649-bb29-612a427e1549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
